{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5dbc5967-22cf-40db-8107-474aa9e8db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import chromadb\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.parse_arxiv import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1a5eea-c7cd-4d91-896c-b27074280647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../data/arxiv/arxiv_workloads/res/k1000_gk500_pn50_n20.json\n"
     ]
    }
   ],
   "source": [
    "res_dict = load_json(\"../data/arxiv/arxiv_workloads/res/k1000_gk500_pn50_n20.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8f11d75-bd55-40dc-8ef0-59f33677c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_res = res_dict['0']\n",
    "\n",
    "one_res.keys()\n",
    "\n",
    "tmp = [v['arxiv_abstract'] for k, v in res_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ed1f014-f450-4b68-9b43-a705c5bc7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cc6fc-e751-40d0-a641-c3b090de8ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2966dad-cfb2-4bfe-a958-a4b06b801ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score([1,2,3, 4], [1,2,104, 4], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3ccac9-d19e-4a2a-bf79-3776dbd2514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = recall_score(one_res['arxiv_abstract'], one_res['arxiv_title'], average=\"micro\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fc013b-47ac-4c48-8405-adcbe1bb403b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = recall_score(one_res['arxiv_abstract'], one_res['hybrid'], average=\"micro\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26294ed7-71a2-42d5-9030-916dbaa0eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = recall_score(one_res['arxiv_abstract'], one_res['weighted_hybrid'], average=\"micro\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0697bd43-158e-49a2-a5aa-0dbe269fc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percent_include(gt, pred, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the percentage of ground truth that is included in the prediction\n",
    "\n",
    "    Note: len(gt) and len(pred) doesn't have to match, but when normalize=True, we used the short length for normalization\n",
    "    \n",
    "    args: \n",
    "        - gt: list\n",
    "        - pred: list\n",
    "        - normalize: bool\n",
    "    \"\"\"\n",
    "    # TP\n",
    "    ret = sum([1 for p in pred if p in gt])\n",
    "    if normalize:\n",
    "        len_nor = min(len(gt), len(pred))\n",
    "        ret = ret / len_nor\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efa018d4-7caa-4f30-a264-3aa81ba643b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_percent_include(one_res['arxiv_abstract'], one_res['arxiv_title'][-500:], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c80618-a2d6-4126-a587-8856f6e80843",
   "metadata": {},
   "source": [
    "# Dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d488c782-9dff-483a-810e-f565436312fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "def get_ids_embed_dict(db_res_obj):\n",
    "    ids_embed_dict = {}\n",
    "    for i, (id, embed) in enumerate(zip(db_res_obj['ids'], db_res_obj['embeddings'])):\n",
    "        ids_embed_dict[str(id)] = embed\n",
    "    return ids_embed_dict\n",
    "\n",
    "def stack_embed_inorder(res_id_list, ids_embed_dict, fill=True, verbose=False):\n",
    "    embed_list = []\n",
    "    if len(res_id_list) != len(ids_embed_dict):\n",
    "        print(\"size mismatch, {} vs {}\".format(len(res_id_list), len(ids_embed_dict)))\n",
    "        if np.abs( len(res_id_list) - len(ids_embed_dict) ) > 1:\n",
    "            # if the size mismatch is too large, return False\n",
    "            return False\n",
    "\n",
    "    for i, id in enumerate(res_id_list):\n",
    "        id = str(id)\n",
    "        if id not in ids_embed_dict:\n",
    "            if verbose: print(i, id)\n",
    "            if fill:\n",
    "                id = res_id_list[i-1]\n",
    "            else:\n",
    "                return False\n",
    "        embed_list.append(ids_embed_dict[id])\n",
    "    return np.array(embed_list)\n",
    "\n",
    "def compute_distance_metrics(gt, pred, gt_collection):\n",
    "    gt_results = gt_collection.get(ids=gt, include=[\"embeddings\"])\n",
    "    pred_results = gt_collection.get(ids=pred, include=[\"embeddings\"])\n",
    "    \n",
    "    # Cosine similarity\n",
    "    gt_id_embed_dict = get_ids_embed_dict(gt_results)\n",
    "    pred_id_embed_dict = get_ids_embed_dict(pred_results)\n",
    "    gt_results = stack_embed_inorder(gt, gt_id_embed_dict)\n",
    "    pred_results = stack_embed_inorder(pred, pred_id_embed_dict)\n",
    "    if gt_results is False:\n",
    "        print(\"size mismatch for gt\")\n",
    "        return None\n",
    "\n",
    "    if pred_results is False:\n",
    "        print(\"size mismatch for pred\")\n",
    "        return None\n",
    "\n",
    "    distances = [cosine_similarity(g, p) for g, p in zip(gt_results, pred_results)]\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4f493df-57e2-47ac-9a4e-c84ce7145fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open(\"../data/arxiv/filtered_data.pickle\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e589847-b97c-410c-8007-1efdd8fd64c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>cat_freq</th>\n",
       "      <th>journal_freq</th>\n",
       "      <th>date_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602903</th>\n",
       "      <td>id_1503.00189</td>\n",
       "      <td>Stefano Pirandola</td>\n",
       "      <td>Shabir Barzanjeh, Saikat Guha, Christian Weedb...</td>\n",
       "      <td>Microwave Quantum Illumination</td>\n",
       "      <td>Main Letter. See arXiv:1410.4008 for an extend...</td>\n",
       "      <td>Phys. Rev. Lett. 114, 080503 (2015)</td>\n",
       "      <td>10.1103/PhysRevLett.114.080503</td>\n",
       "      <td>None</td>\n",
       "      <td>quant-ph cond-mat.other physics.ins-det physic...</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>Quantum illumination is a quantum-optical se...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 28 Feb 201...</td>\n",
       "      <td>2015-03-03</td>\n",
       "      <td>[[Barzanjeh, Shabir, ], [Guha, Saikat, ], [Wee...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id          submitter  \\\n",
       "602903  id_1503.00189  Stefano Pirandola   \n",
       "\n",
       "                                                  authors  \\\n",
       "602903  Shabir Barzanjeh, Saikat Guha, Christian Weedb...   \n",
       "\n",
       "                                 title  \\\n",
       "602903  Microwave Quantum Illumination   \n",
       "\n",
       "                                                 comments  \\\n",
       "602903  Main Letter. See arXiv:1410.4008 for an extend...   \n",
       "\n",
       "                                journal-ref                             doi  \\\n",
       "602903  Phys. Rev. Lett. 114, 080503 (2015)  10.1103/PhysRevLett.114.080503   \n",
       "\n",
       "       report-no                                         categories  \\\n",
       "602903      None  quant-ph cond-mat.other physics.ins-det physic...   \n",
       "\n",
       "                                                  license  \\\n",
       "602903  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "\n",
       "                                                 abstract  \\\n",
       "602903    Quantum illumination is a quantum-optical se...   \n",
       "\n",
       "                                                 versions update_date  \\\n",
       "602903  [{'version': 'v1', 'created': 'Sat, 28 Feb 201...  2015-03-03   \n",
       "\n",
       "                                           authors_parsed  cat_freq  \\\n",
       "602903  [[Barzanjeh, Shabir, ], [Guha, Saikat, ], [Wee...       2.0   \n",
       "\n",
       "        journal_freq  date_freq  \n",
       "602903           2.0      503.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['id']==\"id_1503.00189\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcd5c71e-f904-40d4-842b-816698c992dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arxiv_dp(df_row):\n",
    "    print(df_row['id'].values)\n",
    "    print(df_row['title'].values)\n",
    "    print(df_row['abstract'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78491075-dd9b-4b8b-90ac-1f519d2122e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_1208.2808']\n",
      "['Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster\\n  crawling']\n",
      "['  The growth of world-wide-web (WWW) spreads its wings from an intangible\\nquantities of web-pages to a gigantic hub of web information which gradually\\nincreases the complexity of crawling process in a search engine. A search\\nengine handles a lot of queries from various parts of this world, and the\\nanswers of it solely depend on the knowledge that it gathers by means of\\ncrawling. The information sharing becomes a most common habit of the society,\\nand it is done by means of publishing structured, semi-structured and\\nunstructured resources on the web. This social practice leads to an exponential\\ngrowth of web-resource, and hence it became essential to crawl for continuous\\nupdating of web-knowledge and modification of several existing resources in any\\nsituation. In this paper one statistical hypothesis based learning mechanism is\\nincorporated for learning the behavior of crawling speed in different\\nenvironment of network, and for intelligently control of the speed of crawler.\\nThe scaling technique is used to compare the performance proposed method with\\nthe standard crawler. The high speed performance is observed after scaling, and\\nthe retrieval of relevant web-resource in such a high speed is analyzed.\\n']\n"
     ]
    }
   ],
   "source": [
    "print_arxiv_dp(data[data['id']==\"id_1208.2808\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5092032-0d66-4810-b798-6726831c7057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=arxiv_title), Collection(name=arxiv_abstract)]\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_dbs/\")\n",
    "pprint(chroma_client.list_collections())\n",
    "collection = chroma_client.get_collection(name=\"arxiv_abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "692390f2-b7b8-405d-8a4e-f261aeeeae3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get(ids=\"id_1503.00189\")\n",
    "\n",
    "collection.get(ids=\"id_1311.5047\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26c9d979-5956-4e99-8312-c8285288c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 13, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b2cb5b2-3639-42ee-a9df-b36df3f0b88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['id']==\"id_1208.2808\"]['abstract'].values[0][:2]==\"  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc98b504-dfef-4d78-b9d7-8c734fac0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster crawling'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_str = data[data['id']==\"id_1208.2808\"]['title'].values[0]\n",
    "abstract_str = ' '.join(abstract_str.split())\n",
    "abstract_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d99d7777-492d-47ed-a1ce-772cec7e8f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The growth of world-wide-web (WWW) spreads its wings from an intangible quantities of web-pages to a gigantic hub of web information which gradually increases the complexity of crawling process in a search engine. A search engine handles a lot of queries from various parts of this world, and the answers of it solely depend on the knowledge that it gathers by means of crawling. The information sharing becomes a most common habit of the society, and it is done by means of publishing structured, semi-structured and unstructured resources on the web. This social practice leads to an exponential growth of web-resource, and hence it became essential to crawl for continuous updating of web-knowledge and modification of several existing resources in any situation. In this paper one statistical hypothesis based learning mechanism is incorporated for learning the behavior of crawling speed in different environment of network, and for intelligently control of the speed of crawler. The scaling technique is used to compare the performance proposed method with the standard crawler. The high speed performance is observed after scaling, and the retrieval of relevant web-resource in such a high speed is analyzed.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f50cbde7-2628-4dcf-8b4e-11653cdbebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', 's', 'as', 'as', 'as', 'das', 'd', 'asd']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"as s as as as das d\\nasd\\n\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1285908-556c-4c55-b56f-d60a0be39937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
