{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdb435b-1ab6-4177-bbcf-66d57310e68c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd8f8f9-54c7-4cb1-aac7-de7aa461531d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=title_vector), Collection(name=abstract_vector)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"../data/filtered_data.pickle\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "# chroma_client = chromadb.Client()\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_dbs/\")\n",
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e436d152-a27f-4b1b-9e2a-17c9d3965d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>cat_freq</th>\n",
       "      <th>journal_freq</th>\n",
       "      <th>date_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>704.002</td>\n",
       "      <td>Patrick Roudeau</td>\n",
       "      <td>The BABAR Collaboration, B. Aubert, et al</td>\n",
       "      <td>Measurement of the Hadronic Form Factor in D0 ...</td>\n",
       "      <td>21 pages, 13 postscript figures, submitted to ...</td>\n",
       "      <td>Phys.Rev.D76:052005,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.052005</td>\n",
       "      <td>BABAR-PUB-07/015, SLAC-PUB-12417</td>\n",
       "      <td>hep-ex</td>\n",
       "      <td></td>\n",
       "      <td>The shape of the hadronic form factor f+(q2)...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>[[The BABAR Collaboration, , ], [Aubert, B., ]]</td>\n",
       "      <td>17311</td>\n",
       "      <td>2.0</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>704.2033</td>\n",
       "      <td>Anwar Shiekh Dr.</td>\n",
       "      <td>A.Y. Shiekh</td>\n",
       "      <td>The Quantum Interference Computer: an experime...</td>\n",
       "      <td>6 pages, 1 figure</td>\n",
       "      <td>Int. Jour. of Theo. Phys., 47, 2176, 2008</td>\n",
       "      <td>10.1007/s10773-008-9664-7</td>\n",
       "      <td></td>\n",
       "      <td>quant-ph</td>\n",
       "      <td></td>\n",
       "      <td>An experiment is proposed to test the interf...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 16 Apr 200...</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>[[Shiekh, A. Y., ]]</td>\n",
       "      <td>66088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>704.2088</td>\n",
       "      <td>Shimul Akhanjee</td>\n",
       "      <td>Shimul Akhanjee</td>\n",
       "      <td>Exact longitudinal plasmon dispersion relation...</td>\n",
       "      <td>4 pages, 1 figure. Important typos and errors ...</td>\n",
       "      <td>Phys. Rev. B 76, 165129 (2007)</td>\n",
       "      <td>10.1103/PhysRevB.76.165129</td>\n",
       "      <td></td>\n",
       "      <td>cond-mat.str-el</td>\n",
       "      <td></td>\n",
       "      <td>We derive the exact longitudinal plasmon dis...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 17 Apr 200...</td>\n",
       "      <td>2011-11-09</td>\n",
       "      <td>[[Akhanjee, Shimul, ]]</td>\n",
       "      <td>25554</td>\n",
       "      <td>2.0</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id         submitter                                    authors  \\\n",
       "19     704.002   Patrick Roudeau  The BABAR Collaboration, B. Aubert, et al   \n",
       "2032  704.2033  Anwar Shiekh Dr.                                A.Y. Shiekh   \n",
       "2087  704.2088   Shimul Akhanjee                            Shimul Akhanjee   \n",
       "\n",
       "                                                  title  \\\n",
       "19    Measurement of the Hadronic Form Factor in D0 ...   \n",
       "2032  The Quantum Interference Computer: an experime...   \n",
       "2087  Exact longitudinal plasmon dispersion relation...   \n",
       "\n",
       "                                               comments  \\\n",
       "19    21 pages, 13 postscript figures, submitted to ...   \n",
       "2032                                  6 pages, 1 figure   \n",
       "2087  4 pages, 1 figure. Important typos and errors ...   \n",
       "\n",
       "                                    journal-ref                         doi  \\\n",
       "19                     Phys.Rev.D76:052005,2007  10.1103/PhysRevD.76.052005   \n",
       "2032  Int. Jour. of Theo. Phys., 47, 2176, 2008   10.1007/s10773-008-9664-7   \n",
       "2087             Phys. Rev. B 76, 165129 (2007)  10.1103/PhysRevB.76.165129   \n",
       "\n",
       "                             report-no       categories license  \\\n",
       "19    BABAR-PUB-07/015, SLAC-PUB-12417           hep-ex           \n",
       "2032                                           quant-ph           \n",
       "2087                                    cond-mat.str-el           \n",
       "\n",
       "                                               abstract  \\\n",
       "19      The shape of the hadronic form factor f+(q2)...   \n",
       "2032    An experiment is proposed to test the interf...   \n",
       "2087    We derive the exact longitudinal plasmon dis...   \n",
       "\n",
       "                                               versions update_date  \\\n",
       "19    [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2015-06-30   \n",
       "2032  [{'version': 'v1', 'created': 'Mon, 16 Apr 200...  2009-11-13   \n",
       "2087  [{'version': 'v1', 'created': 'Tue, 17 Apr 200...  2011-11-09   \n",
       "\n",
       "                                       authors_parsed  cat_freq  journal_freq  \\\n",
       "19    [[The BABAR Collaboration, , ], [Aubert, B., ]]     17311           2.0   \n",
       "2032                              [[Shiekh, A. Y., ]]     66088           2.0   \n",
       "2087                           [[Akhanjee, Shimul, ]]     25554           2.0   \n",
       "\n",
       "      date_freq  \n",
       "19          536  \n",
       "2032      25981  \n",
       "2087        992  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(\"\", inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6895e3bf-9e00-41b8-b7b2-459120aba1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"abstract\",\n",
    "        \"authors\",\n",
    "        \"journal-ref\",\n",
    "        \"categories\",\n",
    "        \"comments\",\n",
    "        \"update_date\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "data.drop_duplicates(subset=\"id\", inplace=True)\n",
    "data.drop_duplicates(subset=\"title\", inplace=True)\n",
    "\n",
    "documents = list(data[\"title\"].values)\n",
    "ids = list(data[\"id\"].astype(\"str\").values)\n",
    "metedata = list(data.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49cd52ad-ad8e-4777-9162-9cd155ec6ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"arxiv_collection\")\n",
    "# chroma_client.get_collection(name=\"arxiv_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e4142a-81f7-4739-ba1e-d1e5dcd43a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 7/26 [02:46<07:33, 23.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ids), batch_size)):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]], # could add embed if they are already computed!\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetedata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:147\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:587\u001b[0m, in \u001b[0;36mCollection._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m     )\n\u001b[1;32m--> 587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\chromadb\\utils\\embedding_functions.py:391\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_model_if_not_exists()\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_model_and_tokenizer()\n\u001b[1;32m--> 391\u001b[0m res \u001b[38;5;241m=\u001b[39m cast(Embeddings, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\chromadb\\utils\\embedding_functions.py:340\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[1;34m(self, documents, batch_size)\u001b[0m\n\u001b[0;32m    331\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encoded])\n\u001b[0;32m    332\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(input_ids, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(attention_mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     ),\n\u001b[0;32m    339\u001b[0m }\n\u001b[1;32m--> 340\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Perform mean pooling with attention weighting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\Programming\\VectorDB_playground\\venv\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "for i in tqdm(range(0, len(ids), batch_size)):\n",
    "    collection.add(\n",
    "        # embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]], # could add embed if they are already computed!\n",
    "        documents=documents[i : i + batch_size],\n",
    "        metadatas=metedata[i : i + batch_size],\n",
    "        ids=ids[i : i + batch_size],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577fcc7a-6eb4-46e8-a400-6f1e51579083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\n",
    "        \"Topic Space Trajectories: A case study on machine learning literature\"\n",
    "    ],\n",
    "    n_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d6cbd52-6d40-4a17-9d96-94409c91e912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': None,\n",
      " 'distances': [[0.0, 0.9720746874809265, 0.9970589280128479]],\n",
      " 'documents': [['Topic Space Trajectories: A case study on machine learning '\n",
      "                'literature',\n",
      "                'Leveraging Large Language Models for Topic Classification in '\n",
      "                'the Domain\\n'\n",
      "                '  of Public Affairs',\n",
      "                'BATS: A Spectral Biclustering Approach to Single Document '\n",
      "                'Topic Modeling\\n'\n",
      "                '  and Segmentation']],\n",
      " 'embeddings': None,\n",
      " 'ids': [['2010.12294', '2306.02864', '2008.02218']],\n",
      " 'metadatas': [[{'abstract': '  The annual number of publications at '\n",
      "                             'scientific venues, for example,\\n'\n",
      "                             'conferences and journals, is growing quickly. '\n",
      "                             'Hence, even for researchers it\\n'\n",
      "                             'becomes harder and harder to keep track of '\n",
      "                             'research topics and their progress.\\n'\n",
      "                             'In this task, researchers can be supported by '\n",
      "                             'automated publication analysis.\\n'\n",
      "                             'Yet, many such methods result in '\n",
      "                             'uninterpretable, purely numerical\\n'\n",
      "                             'representations. As an attempt to support human '\n",
      "                             'analysts, we present topic\\n'\n",
      "                             'space trajectories, a structure that allows for '\n",
      "                             'the comprehensible tracking of\\n'\n",
      "                             'research topics. We demonstrate how these '\n",
      "                             'trajectories can be interpreted based\\n'\n",
      "                             'on eight different analysis approaches. To '\n",
      "                             'obtain comprehensible results, we\\n'\n",
      "                             'employ non-negative matrix factorization as well '\n",
      "                             'as suitable visualization\\n'\n",
      "                             'techniques. We show the applicability of our '\n",
      "                             'approach on a publication corpus\\n'\n",
      "                             'spanning 50 years of machine learning research '\n",
      "                             'from 32 publication venues. Our\\n'\n",
      "                             'novel analysis method may be employed for paper '\n",
      "                             'classification, for the\\n'\n",
      "                             'prediction of future research topics, and for '\n",
      "                             'the recommendation of fitting\\n'\n",
      "                             'conferences and journals for submitting '\n",
      "                             'unpublished work.\\n',\n",
      "                 'authors': 'Bastian Sch\\\\\"afermeier and Gerd Stumme and Tom '\n",
      "                            'Hanika',\n",
      "                 'categories': 'cs.LG cs.DL',\n",
      "                 'comments': '41 pages, 8 figures',\n",
      "                 'id': 2010.12294,\n",
      "                 'journal-ref': 'Scientometrics (2021)',\n",
      "                 'title': 'Topic Space Trajectories: A case study on machine '\n",
      "                          'learning literature',\n",
      "                 'update_date': '2021-05-19'},\n",
      "                {'abstract': '  The analysis of public affairs documents is '\n",
      "                             'crucial for citizens as it\\n'\n",
      "                             'promotes transparency, accountability, and '\n",
      "                             'informed decision-making. It allows\\n'\n",
      "                             'citizens to understand government policies, '\n",
      "                             'participate in public discourse,\\n'\n",
      "                             'and hold representatives accountable. This is '\n",
      "                             'crucial, and sometimes a matter\\n'\n",
      "                             'of life or death, for companies whose operation '\n",
      "                             'depend on certain regulations.\\n'\n",
      "                             'Large Language Models (LLMs) have the potential '\n",
      "                             'to greatly enhance the analysis\\n'\n",
      "                             'of public affairs documents by effectively '\n",
      "                             'processing and understanding the\\n'\n",
      "                             'complex language used in such documents. In this '\n",
      "                             'work, we analyze the\\n'\n",
      "                             'performance of LLMs in classifying public '\n",
      "                             'affairs documents. As a natural\\n'\n",
      "                             'multi-label task, the classification of these '\n",
      "                             'documents presents important\\n'\n",
      "                             'challenges. In this work, we use a regex-powered '\n",
      "                             'tool to collect a database of\\n'\n",
      "                             'public affairs documents with more than 33K '\n",
      "                             'samples and 22.5M tokens. Our\\n'\n",
      "                             'experiments assess the performance of 4 '\n",
      "                             'different Spanish LLMs to classify up\\n'\n",
      "                             'to 30 different topics in the data in different '\n",
      "                             'configurations. The results\\n'\n",
      "                             'shows that LLMs can be of great use to process '\n",
      "                             'domain-specific documents, such\\n'\n",
      "                             'as those in the domain of public affairs.\\n',\n",
      "                 'authors': 'Alejandro Pe\\\\~na, Aythami Morales, Julian '\n",
      "                            'Fierrez, Ignacio Serna,\\n'\n",
      "                            '  Javier Ortega-Garcia, I\\\\~nigo Puente, Jorge '\n",
      "                            'Cordova, Gonzalo Cordova',\n",
      "                 'categories': 'cs.AI cs.CL',\n",
      "                 'comments': 'Accepted in ICDAR 2023 Workshop on Automatic '\n",
      "                             'Domain-Adapted and\\n'\n",
      "                             '  Personalized Document Analysis',\n",
      "                 'id': 2306.02864,\n",
      "                 'journal-ref': 'Document Analysis and Recognition - ICDAR '\n",
      "                                '2023 Workshops. ICDAR\\n'\n",
      "                                '  2023. Lecture Notes in Computer Science, '\n",
      "                                'vol 14194',\n",
      "                 'title': 'Leveraging Large Language Models for Topic '\n",
      "                          'Classification in the Domain\\n'\n",
      "                          '  of Public Affairs',\n",
      "                 'update_date': '2023-09-06'},\n",
      "                {'abstract': '  Existing topic modeling and text segmentation '\n",
      "                             'methodologies generally require\\n'\n",
      "                             'large datasets for training, limiting their '\n",
      "                             'capabilities when only small\\n'\n",
      "                             'collections of text are available. In this work, '\n",
      "                             'we reexamine the inter-related\\n'\n",
      "                             'problems of \"topic identification\" and \"text '\n",
      "                             'segmentation\" for sparse document\\n'\n",
      "                             'learning, when there is a single new text of '\n",
      "                             'interest. In developing a\\n'\n",
      "                             'methodology to handle single documents, we face '\n",
      "                             'two major challenges. First is\\n'\n",
      "                             'sparse information: with access to only one '\n",
      "                             'document, we cannot train\\n'\n",
      "                             'traditional topic models or deep learning '\n",
      "                             'algorithms. Second is significant\\n'\n",
      "                             'noise: a considerable portion of words in any '\n",
      "                             'single document will produce only\\n'\n",
      "                             'noise and not help discern topics or segments. '\n",
      "                             'To tackle these issues, we\\n'\n",
      "                             'design an unsupervised, computationally '\n",
      "                             'efficient methodology called BATS:\\n'\n",
      "                             'Biclustering Approach to Topic modeling and '\n",
      "                             'Segmentation. BATS leverages three\\n'\n",
      "                             'key ideas to simultaneously identify topics and '\n",
      "                             'segment text: (i) a new\\n'\n",
      "                             'mechanism that uses word order information to '\n",
      "                             'reduce sample complexity, (ii) a\\n'\n",
      "                             'statistically sound graph-based biclustering '\n",
      "                             'technique that identifies latent\\n'\n",
      "                             'structures of words and sentences, and (iii) a '\n",
      "                             'collection of effective\\n'\n",
      "                             'heuristics that remove noise words and award '\n",
      "                             'important words to further improve\\n'\n",
      "                             'performance. Experiments on four datasets show '\n",
      "                             'that our approach outperforms\\n'\n",
      "                             'several state-of-the-art baselines when '\n",
      "                             'considering topic coherence, topic\\n'\n",
      "                             'diversity, segmentation, and runtime comparison '\n",
      "                             'metrics.\\n',\n",
      "                 'authors': 'Qiong Wu, Adam Hare, Sirui Wang, Yuwei Tu, '\n",
      "                            'Zhenming Liu, Christopher\\n'\n",
      "                            '  G. Brinton, Yanhua Li',\n",
      "                 'categories': 'cs.IR cs.LG',\n",
      "                 'comments': '28 pages',\n",
      "                 'id': 2008.02218,\n",
      "                 'journal-ref': 'ACM Transactions on Intelligent Systems and '\n",
      "                                'Technology, 2021',\n",
      "                 'title': 'BATS: A Spectral Biclustering Approach to Single '\n",
      "                          'Document Topic Modeling\\n'\n",
      "                          '  and Segmentation',\n",
      "                 'update_date': '2021-05-26'}]],\n",
      " 'uris': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5aa73-5d5a-4b77-b49c-851828eada16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
